@article{aikawa2024improving,
  title={Improving the efficiency of training physics-informed neural networks using active learning},
  author={Aikawa, Yuri and Ueda, Naonori and Tanaka, Toshiyuki},
  journal={New Generation Computing},
  pages={1--22},
  year={2024},
  publisher={Springer}
}

@article{hou2023enhancing,
  title={Enhancing PINNs for solving PDEs via adaptive collocation point movement and adaptive loss weighting},
  author={Hou, Jie and Li, Ying and Ying, Shihui},
  journal={Nonlinear Dynamics},
  volume={111},
  number={16},
  pages={15233--15261},
  year={2023},
  publisher={Springer}
}

@misc{münzer2022curriculumtrainingbasedstrategydistributingcollocation,
      title={A Curriculum-Training-Based Strategy for Distributing Collocation Points during Physics-Informed Neural Network Training}, 
      author={Marcus Münzer and Chris Bard},
      year={2022},
      eprint={2211.11396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.11396}, 
}
@misc{matsubara2023goodlatticetrainingphysicsinformed,
      title={Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory}, 
      author={Takashi Matsubara and Takaharu Yaguchi},
      year={2023},
      eprint={2307.13869},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.13869}, 
}
@misc{subramanian2022adaptiveselfsupervisionalgorithmsphysicsinformed,
      title={Adaptive Self-supervision Algorithms for Physics-informed Neural Networks}, 
      author={Shashank Subramanian and Robert M. Kirby and Michael W. Mahoney and Amir Gholami},
      year={2022},
      eprint={2207.04084},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.04084}, 
}

@article{pinkus1999approximation,
  title={Approximation theory of the MLP model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}

@misc{krishnapriyan2021characterizingpossiblefailuremodes,
      title={Characterizing possible failure modes in physics-informed neural networks}, 
      author={Aditi S. Krishnapriyan and Amir Gholami and Shandian Zhe and Robert M. Kirby and Michael W. Mahoney},
      year={2021},
      eprint={2109.01050},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.01050}, 
}

@misc{c2021failure,
  author       = {C.},
  title        = {Possible Failure Modes in Physics-Informed Neural Networks},
  year         = {2021},
  howpublished = {\url{https://github.com/a1k12/characterizing-pinns-failure-modes}}
}

@article{lu2021deepxde,
  author  = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  title   = {{DeepXDE}: A deep learning library for solving differential equations},
  journal = {SIAM Review},
  volume  = {63},
  number  = {1},
  pages   = {208-228},
  year    = {2021},
  doi     = {10.1137/19M1274067}
}

@misc{grossmann2023physicsinformedneuralnetworksbeat,
      title={Can Physics-Informed Neural Networks beat the Finite Element Method?}, 
      author={Tamara G. Grossmann and Urszula Julia Komorowska and Jonas Latz and Carola-Bibiane Schönlieb},
      year={2023},
      eprint={2302.04107},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2302.04107}, 
}

@misc{luo2023residualminimizationpdesfailure,
      title={On Residual Minimization for PDEs: Failure of PINN, Modified Equation, and Implicit Bias}, 
      author={Tao Luo and Qixuan Zhou},
      year={2023},
      eprint={2310.18201},
      archivePrefix={arXiv},
      primaryClass={math.AP},
      url={https://arxiv.org/abs/2310.18201}, 
}

@misc{e2017deepritzmethoddeep,
      title={The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems}, 
      author={Weinan E and Bing Yu},
      year={2017},
      eprint={1710.00211},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1710.00211}, 
}

@article{deeplearningSIAM,
author = {Higham, Catherine F. and Higham, Desmond J.},
title = {Deep Learning: An Introduction for Applied Mathematicians},
journal = {SIAM Review},
volume = {61},
number = {4},
pages = {860-891},
year = {2019},
doi = {10.1137/18M1165748},
URL = { 
        https://doi.org/10.1137/18M1165748
},
eprint = { 
        https://doi.org/10.1137/18M1165748
}
,
    abstract = { Abstract. Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics, notably from calculus, approximation theory, optimization, and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: What is a deep neural network? How is a network trained? What is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also demonstrate the use of state-of-the-art software on a large scale image classification problem. We finish with references to the current literature. }
}

