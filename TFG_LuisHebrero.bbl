% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nty/global//global/global}
  \entry{aikawa2024improving}{article}{}
    \name{author}{3}{}{%
      {{hash=AY}{%
         family={Aikawa},
         familyi={A\bibinitperiod},
         given={Yuri},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=UN}{%
         family={Ueda},
         familyi={U\bibinitperiod},
         given={Naonori},
         giveni={N\bibinitperiod},
      }}%
      {{hash=TT}{%
         family={Tanaka},
         familyi={T\bibinitperiod},
         given={Toshiyuki},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{AYUNTT1}
    \strng{fullhash}{AYUNTT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{pages}{1\bibrangedash 22}
    \field{title}{Improving the efficiency of training physics-informed neural networks using active learning}
    \field{journaltitle}{New Generation Computing}
    \field{year}{2024}
  \endentry

  \entry{e2017deepritzmethoddeep}{article}{}
    \name{author}{2}{}{%
      {{hash=EW}{%
         family={E},
         familyi={E},
         given={Weinan},
         giveni={W\bibinitperiod},
      }}%
      {{hash=YB}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Bing},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{EWYB1}
    \strng{fullhash}{EWYB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{number}{1}
    \field{pages}{1\bibrangedash 12}
    \field{title}{The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems}
    \verb{url}
    \verb https://doi.org/10.48550/arXiv.1710.00211
    \endverb
    \field{volume}{6}
    \field{journaltitle}{Communications in Mathematics and Statistics}
    \field{year}{2018}
  \endentry

  \entry{deeplearningSIAM}{article}{}
    \name{author}{2}{}{%
      {{hash=HCF}{%
         family={Higham},
         familyi={H\bibinitperiod},
         given={Catherine\bibnamedelima F.},
         giveni={C\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
      {{hash=HDJ}{%
         family={Higham},
         familyi={H\bibinitperiod},
         given={Desmond\bibnamedelima J.},
         giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{HCFHDJ1}
    \strng{fullhash}{HCFHDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Abstract. Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics, notably from calculus, approximation theory, optimization, and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: What is a deep neural network? How is a network trained? What is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also demonstrate the use of state-of-the-art software on a large scale image classification problem. We finish with references to the current literature.%
    }
    \verb{doi}
    \verb 10.1137/18M1165748
    \endverb
    \verb{eprint}
    \verb https://doi.org/10.1137/18M1165748
    \endverb
    \field{number}{4}
    \field{pages}{860\bibrangedash 891}
    \field{title}{Deep Learning: An Introduction for Applied Mathematicians}
    \verb{url}
    \verb https://doi.org/10.1137/18M1165748
    \endverb
    \field{volume}{61}
    \field{journaltitle}{SIAM Review}
    \field{year}{2019}
  \endentry

  \entry{hou2023enhancing}{article}{}
    \name{author}{3}{}{%
      {{hash=HJ}{%
         family={Hou},
         familyi={H\bibinitperiod},
         given={Jie},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Ying},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={Ying},
         familyi={Y\bibinitperiod},
         given={Shihui},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{HJLYYS1}
    \strng{fullhash}{HJLYYS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{number}{16}
    \field{pages}{15233\bibrangedash 15261}
    \field{title}{Enhancing PINNs for solving PDEs via adaptive collocation point movement and adaptive loss weighting}
    \field{volume}{111}
    \field{journaltitle}{Nonlinear Dynamics}
    \field{year}{2023}
  \endentry

  \entry{lu2021deepxde}{article}{}
    \name{author}{4}{}{%
      {{hash=LL}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Lu},
         giveni={L\bibinitperiod},
      }}%
      {{hash=MX}{%
         family={Meng},
         familyi={M\bibinitperiod},
         given={Xuhui},
         giveni={X\bibinitperiod},
      }}%
      {{hash=MZ}{%
         family={Mao},
         familyi={M\bibinitperiod},
         given={Zhiping},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=KGE}{%
         family={Karniadakis},
         familyi={K\bibinitperiod},
         given={George\bibnamedelima Em},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \strng{namehash}{LL+1}
    \strng{fullhash}{LLMXMZKGE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \verb{doi}
    \verb 10.1137/19M1274067
    \endverb
    \field{number}{1}
    \field{pages}{208\bibrangedash 228}
    \field{title}{{DeepXDE}: A deep learning library for solving differential equations}
    \field{volume}{63}
    \field{journaltitle}{SIAM Review}
    \field{year}{2021}
  \endentry

  \entry{luo2023residualminimizationpdesfailure}{misc}{}
    \name{author}{2}{}{%
      {{hash=LT}{%
         family={Luo},
         familyi={L\bibinitperiod},
         given={Tao},
         giveni={T\bibinitperiod},
      }}%
      {{hash=ZQ}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Qixuan},
         giveni={Q\bibinitperiod},
      }}%
    }
    \strng{namehash}{LTZQ1}
    \strng{fullhash}{LTZQ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \verb{eprint}
    \verb 2310.18201
    \endverb
    \field{title}{On Residual Minimization for PDEs: Failure of PINN, Modified Equation, and Implicit Bias}
    \verb{url}
    \verb https://arxiv.org/abs/2310.18201
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{math.AP}
    \field{year}{2023}
  \endentry

  \entry{matsubara2023goodlatticetrainingphysicsinformed}{misc}{}
    \name{author}{2}{}{%
      {{hash=MT}{%
         family={Matsubara},
         familyi={M\bibinitperiod},
         given={Takashi},
         giveni={T\bibinitperiod},
      }}%
      {{hash=YT}{%
         family={Yaguchi},
         familyi={Y\bibinitperiod},
         given={Takaharu},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{MTYT1}
    \strng{fullhash}{MTYT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \verb{eprint}
    \verb 2307.13869
    \endverb
    \field{title}{Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory}
    \verb{url}
    \verb https://arxiv.org/abs/2307.13869
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{year}{2023}
  \endentry

  \entry{münzer2022curriculumtrainingbasedstrategydistributingcollocation}{misc}{}
    \name{author}{2}{}{%
      {{hash=MM}{%
         family={Münzer},
         familyi={M\bibinitperiod},
         given={Marcus},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Bard},
         familyi={B\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{MMBC1}
    \strng{fullhash}{MMBC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \verb{eprint}
    \verb 2211.11396
    \endverb
    \field{title}{A Curriculum-Training-Based Strategy for Distributing Collocation Points during Physics-Informed Neural Network Training}
    \verb{url}
    \verb https://arxiv.org/abs/2211.11396
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{year}{2022}
  \endentry

  \entry{pinkus1999approximation}{article}{}
    \name{author}{1}{}{%
      {{hash=PA}{%
         family={Pinkus},
         familyi={P\bibinitperiod},
         given={Allan},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \strng{namehash}{PA1}
    \strng{fullhash}{PA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{pages}{143\bibrangedash 195}
    \field{title}{Approximation theory of the MLP model in neural networks}
    \field{volume}{8}
    \field{journaltitle}{Acta numerica}
    \field{year}{1999}
  \endentry

  \entry{subramanian2022adaptiveselfsupervisionalgorithmsphysicsinformed}{misc}{}
    \name{author}{4}{}{%
      {{hash=SS}{%
         family={Subramanian},
         familyi={S\bibinitperiod},
         given={Shashank},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KRM}{%
         family={Kirby},
         familyi={K\bibinitperiod},
         given={Robert\bibnamedelima M.},
         giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=MMW}{%
         family={Mahoney},
         familyi={M\bibinitperiod},
         given={Michael\bibnamedelima W.},
         giveni={M\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gholami},
         familyi={G\bibinitperiod},
         given={Amir},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{SS+1}
    \strng{fullhash}{SSKRMMMWGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \verb{eprint}
    \verb 2207.04084
    \endverb
    \field{title}{Adaptive Self-supervision Algorithms for Physics-informed Neural Networks}
    \verb{url}
    \verb https://arxiv.org/abs/2207.04084
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{year}{2022}
  \endentry
\enddatalist
\endinput
